{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "490e2992",
      "metadata": {
        "id": "490e2992"
      },
      "source": [
        "\n",
        "# ğŸ“— Notebook 2: Semantic Similarity and Vector Closeness\n",
        "\n",
        "This notebook explores how **embeddings help compare the meaning of sentences**.\n",
        "\n",
        "**Semantic similarity** is about comparing the *meaning* of two pieces of text â€” not just whether they use the same words.\n",
        "\n",
        "Examples:\n",
        "- â€œThe dog barked loudly.â€\n",
        "- â€œThe canine made noise.â€\n",
        "\n",
        "These donâ€™t share many exact words, but mean nearly the same thing. A basic keyword match wouldnâ€™t see the connection â€” but a language model does, by encoding meaning in vectors.\n",
        "\n",
        "When a model like `SentenceTransformer` reads a sentence, it creates an **embedding** â€” a list of numbers that represents the sentenceâ€™s meaning. These embeddings are **vectors in a high-dimensional space**.\n",
        "\n",
        "### ğŸ§² Analogy:\n",
        "- Each sentence is a dot in a big space.\n",
        "- Sentences with similar meaning are nearby dots.\n",
        "- The distance between them tells us how semantically similar they are.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5de1bee",
      "metadata": {
        "id": "a5de1bee"
      },
      "source": [
        "\n",
        "### ğŸ“ How Do We Measure Closeness?\n",
        "\n",
        "The most common method is **cosine similarity**. It measures the *angle* between two vectors.\n",
        "\n",
        "- Cosine similarity = 1.0 â†’ exactly the same direction (perfect match)\n",
        "- â‰ˆ 0.7â€“0.9 â†’ similar meaning\n",
        "- = 0.0 â†’ no similarity\n",
        "- < 0 â†’ opposite meanings (rare with sentence embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e7c065c3",
      "metadata": {
        "id": "e7c065c3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging\n",
        "import warnings\n",
        "from transformers.utils import logging as hf_logging\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Suppress all user warnings, including HF_TOKEN ones\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Suppress Hugging Face logs\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "hf_logging.set_verbosity_error()\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1644b6e3",
      "metadata": {
        "id": "1644b6e3"
      },
      "source": [
        "\n",
        "## ğŸ” Quick Warm-Up Example\n",
        "\n",
        "A simple test of three sentences. We expect sentences 1 and 2 to be close in meaning, and sentence 3 to differ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bfaaff",
      "metadata": {
        "id": "e7bfaaff"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentences = [\n",
        "    \"The cat sits on the mat.\",\n",
        "    \"A feline is on a rug.\",\n",
        "    \"I went to the market.\"\n",
        "]\n",
        "embeddings = model.encode(sentences)\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Display similarity matrix\n",
        "pd.DataFrame(similarity_matrix, index=sentences, columns=sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e107c0f",
      "metadata": {
        "id": "6e107c0f"
      },
      "source": [
        "\n",
        "## ğŸ§  Semantic Clustering of Expanded Sentences\n",
        "\n",
        "Now letâ€™s explore 6 more varied sentences and see how similar topics cluster together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c27231d",
      "metadata": {
        "id": "0c27231d"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentences = [\n",
        "    \"The stock market crashed in 2008.\",\n",
        "    \"Financial markets collapsed in the late 2000s.\",\n",
        "    \"My dog loves playing fetch in the park.\",\n",
        "    \"Throwing a ball for my puppy at the park is fun.\",\n",
        "    \"I had toast for breakfast today.\",\n",
        "    \"Breakfast was just some plain white bread.\"\n",
        "]\n",
        "vectors = model.encode(sentences)\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "reduced = PCA(n_components=2).fit_transform(vectors)\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    plt.scatter(reduced[i, 0], reduced[i, 1])\n",
        "    plt.annotate(f\"{i+1}. {sentence}\", (reduced[i, 0]+0.01, reduced[i, 1]+0.01), fontsize=9)\n",
        "plt.title(\"2D Visualization of Sentence Embeddings\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b3c1d7",
      "metadata": {
        "id": "42b3c1d7"
      },
      "source": [
        "\n",
        "### ğŸ”¥ Heatmap: Cosine Similarity Between Sentences\n",
        "\n",
        "Visualizing pairwise similarity scores.\n",
        "\n",
        "How to Read the Heatmap\n",
        "* Each cell shows the similarity between a pair of sentences.\n",
        "* he diagonal (top-left to bottom-right) will always show 1.0 because each sentence is perfectly similar to itself.\n",
        "* Lighter blue = higher similarity\n",
        "* Darker blue = lower similarity\n",
        "\n",
        "ğŸ“Œ Why Itâ€™s Useful\n",
        "\n",
        "This helps you:\n",
        "* Spot clusters of sentences with similar meanings.\n",
        "* Identify outliers that are semantically different.\n",
        "* Visually explore which sentences are most alike â€” especially helpful in summarization, clustering, or search applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f74ea3c5",
      "metadata": {
        "id": "f74ea3c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.heatmap(similarity_matrix, annot=True, xticklabels=sentences, yticklabels=sentences, cmap=\"Blues\", fmt=\".2f\")\n",
        "plt.title(\"Cosine Similarity Between Sentences\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}