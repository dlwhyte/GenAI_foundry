{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "490e2992",
      "metadata": {
        "id": "490e2992"
      },
      "source": [
        "\n",
        "# 📗 Notebook 2: Semantic Similarity and Vector Closeness\n",
        "\n",
        "This notebook explores how **embeddings help compare the meaning of sentences**.\n",
        "\n",
        "**Semantic similarity** is about comparing the *meaning* of two pieces of text — not just whether they use the same words.\n",
        "\n",
        "Examples:\n",
        "- “The dog barked loudly.”\n",
        "- “The canine made noise.”\n",
        "\n",
        "These don’t share many exact words, but mean nearly the same thing. A basic keyword match wouldn’t see the connection — but a language model does, by encoding meaning in vectors.\n",
        "\n",
        "When a model like `SentenceTransformer` reads a sentence, it creates an **embedding** — a list of numbers that represents the sentence’s meaning. These embeddings are **vectors in a high-dimensional space**.\n",
        "\n",
        "### 🧲 Analogy:\n",
        "- Each sentence is a dot in a big space.\n",
        "- Sentences with similar meaning are nearby dots.\n",
        "- The distance between them tells us how semantically similar they are.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5de1bee",
      "metadata": {
        "id": "a5de1bee"
      },
      "source": [
        "\n",
        "### 📐 How Do We Measure Closeness?\n",
        "\n",
        "The most common method is **cosine similarity**. It measures the *angle* between two vectors.\n",
        "\n",
        "- Cosine similarity = 1.0 → exactly the same direction (perfect match)\n",
        "- ≈ 0.7–0.9 → similar meaning\n",
        "- = 0.0 → no similarity\n",
        "- < 0 → opposite meanings (rare with sentence embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e7c065c3",
      "metadata": {
        "id": "e7c065c3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging\n",
        "import warnings\n",
        "from transformers.utils import logging as hf_logging\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Suppress all user warnings, including HF_TOKEN ones\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Suppress Hugging Face logs\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "hf_logging.set_verbosity_error()\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1644b6e3",
      "metadata": {
        "id": "1644b6e3"
      },
      "source": [
        "\n",
        "## 🔍 Quick Warm-Up Example\n",
        "\n",
        "A simple test of three sentences. We expect sentences 1 and 2 to be close in meaning, and sentence 3 to differ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bfaaff",
      "metadata": {
        "id": "e7bfaaff"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentences = [\n",
        "    \"The cat sits on the mat.\",\n",
        "    \"A feline is on a rug.\",\n",
        "    \"I went to the market.\"\n",
        "]\n",
        "embeddings = model.encode(sentences)\n",
        "similarity_matrix = cosine_similarity(embeddings)\n",
        "\n",
        "# Display similarity matrix\n",
        "pd.DataFrame(similarity_matrix, index=sentences, columns=sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e107c0f",
      "metadata": {
        "id": "6e107c0f"
      },
      "source": [
        "\n",
        "## 🧠 Semantic Clustering of Expanded Sentences\n",
        "\n",
        "Now let’s explore 6 more varied sentences and see how similar topics cluster together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c27231d",
      "metadata": {
        "id": "0c27231d"
      },
      "outputs": [],
      "source": [
        "\n",
        "sentences = [\n",
        "    \"The stock market crashed in 2008.\",\n",
        "    \"Financial markets collapsed in the late 2000s.\",\n",
        "    \"My dog loves playing fetch in the park.\",\n",
        "    \"Throwing a ball for my puppy at the park is fun.\",\n",
        "    \"I had toast for breakfast today.\",\n",
        "    \"Breakfast was just some plain white bread.\"\n",
        "]\n",
        "vectors = model.encode(sentences)\n",
        "similarity_matrix = cosine_similarity(vectors)\n",
        "reduced = PCA(n_components=2).fit_transform(vectors)\n",
        "\n",
        "# Scatter plot\n",
        "plt.figure(figsize=(10, 7))\n",
        "for i, sentence in enumerate(sentences):\n",
        "    plt.scatter(reduced[i, 0], reduced[i, 1])\n",
        "    plt.annotate(f\"{i+1}. {sentence}\", (reduced[i, 0]+0.01, reduced[i, 1]+0.01), fontsize=9)\n",
        "plt.title(\"2D Visualization of Sentence Embeddings\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b3c1d7",
      "metadata": {
        "id": "42b3c1d7"
      },
      "source": [
        "\n",
        "### 🔥 Heatmap: Cosine Similarity Between Sentences\n",
        "\n",
        "Visualizing pairwise similarity scores.\n",
        "\n",
        "How to Read the Heatmap\n",
        "* Each cell shows the similarity between a pair of sentences.\n",
        "* he diagonal (top-left to bottom-right) will always show 1.0 because each sentence is perfectly similar to itself.\n",
        "* Lighter blue = higher similarity\n",
        "* Darker blue = lower similarity\n",
        "\n",
        "📌 Why It’s Useful\n",
        "\n",
        "This helps you:\n",
        "* Spot clusters of sentences with similar meanings.\n",
        "* Identify outliers that are semantically different.\n",
        "* Visually explore which sentences are most alike — especially helpful in summarization, clustering, or search applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f74ea3c5",
      "metadata": {
        "id": "f74ea3c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.heatmap(similarity_matrix, annot=True, xticklabels=sentences, yticklabels=sentences, cmap=\"Blues\", fmt=\".2f\")\n",
        "plt.title(\"Cosine Similarity Between Sentences\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}