{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2f8d71fd",
      "metadata": {
        "id": "2f8d71fd"
      },
      "source": [
        "# üìò Notebook 1: Introduction to Tokens and Embeddings\n",
        "This notebook introduces the basic concepts of tokens and embeddings used in Large Language Models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac75f878",
      "metadata": {
        "id": "ac75f878"
      },
      "source": [
        "## What is an LLM?\n",
        "A Large Language Model (LLM) is a type of AI trained on massive amounts of text to understand and generate human-like language. It doesn‚Äôt ‚Äúknow‚Äù facts ‚Äî instead, it learns patterns, grammar, and meaning through billions of examples.\n",
        "\n",
        "\n",
        "## üß© What is a Token?\n",
        "A token is a piece of text ‚Äî often a word or subword ‚Äî that the model processes. Tokenizers convert sentences into tokens. LLMs don‚Äôt see raw text. They break input into tokens, which are like pieces of words or symbols. Tokenizing helps standardize and compress the input for processing.\n",
        "\n",
        "## üî¢ Tokenization Example: Word vs. Tokens vs. Token IDs\n",
        "\n",
        "Before we get embeddings, we break a sentence into tokens.\n",
        "Here's an example:\n",
        "\n",
        "| Word      | Token(s)     | Token ID(s)     |\n",
        "|-----------|--------------|-----------------|\n",
        "| running   | run, ##ning  | 2142, 2075      |\n",
        "| happily   | happ, ##ily  | 6203, 12973     |\n",
        "\n",
        "- Some words break into **subwords** based on what the model has seen during training.\n",
        "- Each token is mapped to a unique ID from the model's vocabulary.\n",
        "\n",
        "This is what gets fed into the model ‚Äî not the raw text!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e54489",
      "metadata": {
        "id": "15e54489"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import warnings\n",
        "from transformers.utils import logging as hf_logging\n",
        "import os\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Suppress all user warnings, including HF_TOKEN ones\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Suppress Hugging Face progress bars and warnings\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "hf_logging.set_verbosity_error()\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "sentence = \"The cat sat on the mat.\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", tokenizer.convert_tokens_to_ids(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd47691d",
      "metadata": {
        "id": "dd47691d"
      },
      "source": [
        "## üìê What is an Embedding?\n",
        "Once a sentence is tokenized, each token is converted into a vector ‚Äî a list of numbers that captures its meaning. These are called embeddings. They allow machines to compare meaning using math.\n",
        "Embeddings help AI understand meaning, not just words. For example, the sentences:\n",
        "*   \"The dog ran away.\"\n",
        "*   \"The canine escaped.\"\n",
        "\n",
        "‚Ä¶may use different words, but their embeddings are similar because their meaning is similar.\n",
        "\n",
        "The code below takes a sentence written in English and shows you how a computer ‚Äúunderstands‚Äù it ‚Äî by turning it into a long list of numbers called an embedding.\n",
        "* `SentenceTransformer` is a tool that takes a sentence and creates a special kind of fingerprint called an embedding ‚Äî a list of numbers that capture the meaning of the sentence.\n",
        "* `embedding = model.encode([sentence])[0]`\n",
        "This is where your sentence gets converted into a vector ‚Äî a list of around 384 numbers (for this model). Each number represents a different aspect of the sentence‚Äôs meaning (e.g., its tone, topic, grammar, etc.).\n",
        "* The bar chart `(plt.bar(...))`\n",
        "This chart visualizes those numbers in a way your eyes can understand. Each bar shows how ‚Äústrongly‚Äù the sentence expresses a certain hidden concept. The x-axis is the dimension number (just the position in the list), and the y-axis is the value (how much that dimension is ‚Äúactive‚Äù)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ehx5CZNQPfu"
      },
      "outputs": [],
      "source": [
        "\n",
        "import logging\n",
        "import warnings\n",
        "from transformers.utils import logging as hf_logging\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Suppress Hugging Face logs\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "hf_logging.set_verbosity_error()\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "embedding = model.encode([sentence])[0]\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "plt.bar(range(len(embedding)), embedding)\n",
        "plt.title(f\"Embedding Vector for: '{sentence}'\")\n",
        "plt.xlabel(\"Dimension\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.show()\n"
      ],
      "id": "5ehx5CZNQPfu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ Why Do We Use Embeddings?\n",
        "\n",
        "Embeddings turn text into numbers ‚Äî but why does that matter?\n",
        "\n",
        "- **Similar sentences ‚Üí similar vectors** (they are *close* in space).\n",
        "- **Different sentences ‚Üí distant vectors** (they are *far apart* in space).\n",
        "\n",
        "This makes it easy for the computer to understand meaning using math.\n",
        "For example:\n",
        "\n",
        "| Sentence A                  | Sentence B                  | Cosine Similarity |\n",
        "|----------------------------|-----------------------------|-------------------|\n",
        "| The dog barked.            | A canine made noise.        | High              |\n",
        "| I like pizza.              | The economy is slowing down.| Low               |\n",
        "\n",
        "We use embeddings for:\n",
        "- **Semantic Search**: Find texts that *mean* the same thing.\n",
        "- **Chatbots**: Match new questions to known answers.\n",
        "- **Classification**: Label text by analyzing its vector.\n",
        "- **Clustering**: Group similar ideas together."
      ],
      "metadata": {
        "id": "A9WzyPxvRgO1"
      },
      "id": "A9WzyPxvRgO1"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}