{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWwR4GoLwuCR"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# üîß Step 1: Install Required Dependencies\n",
        "# - faiss-cpu: for vector similarity search\n",
        "# - sentence-transformers: to embed text as vectors\n",
        "# - openai: to query GPT models\n",
        "# - python-docx: to parse Word (.docx) files\n",
        "# - PyMuPDF (fitz): to extract text from PDFs\n",
        "# - XetHub is a backend system Hugging Face supports that allows faster and more efficient file downloads\n",
        "# ================================================\n",
        "!pip install -q faiss-cpu sentence-transformers openai python-docx PyMuPDF\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "\n",
        "# ================================================\n",
        "# üîê Step 2: Input your OpenAI API key securely\n",
        "# - We use getpass so the key isn't visible\n",
        "# - Sets the API key in an environment variable\n",
        "# ================================================\n",
        "from getpass import getpass\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suppress unnecessary warnings from Hugging Face (if not using HF tokens)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Ask for OpenAI API key and store it securely\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîê Enter your OpenAI API key: \")\n",
        "\n",
        "# ================================================\n",
        "# üìÅ Step 3: Upload Documents\n",
        "# - Supports .txt, .pdf, and .docx files\n",
        "# - Uses Google Colab‚Äôs file upload interface\n",
        "# ================================================\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Optional: suppress Hugging Face token warnings in Colab\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    message=\"The secret `HF_TOKEN` does not exist in your Colab secrets.\",\n",
        "    category=UserWarning,\n",
        "    module=\"huggingface_hub.utils._auth\"\n",
        ")\n",
        "\n",
        "# ================================================\n",
        "# üìÑ Step 4: Extract Text from Uploaded Files\n",
        "# - PDF: uses PyMuPDF\n",
        "# - DOCX: uses python-docx\n",
        "# - TXT: plain read\n",
        "# ================================================\n",
        "import fitz  # for PDF files\n",
        "import docx  # for Word files\n",
        "\n",
        "# Helper function to extract text based on file type\n",
        "def extract_text_from_file(filename):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    elif filename.endswith(\".pdf\"):\n",
        "        text = \"\"\n",
        "        with fitz.open(filename) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "        return text\n",
        "    elif filename.endswith(\".docx\"):\n",
        "        doc = docx.Document(filename)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    else:\n",
        "        return \"\"  # Unsupported file type\n",
        "\n",
        "# Extract and collect text from uploaded files\n",
        "documents = []\n",
        "for fname in uploaded.keys():\n",
        "    text = extract_text_from_file(fname)\n",
        "    if text:\n",
        "        documents.append(text)\n",
        "\n",
        "# ================================================\n",
        "# ‚úÇÔ∏è Step 5: Chunk the Extracted Text\n",
        "# - Chunks help with LLM context limits\n",
        "# - Default: 500 characters with 50 character overlap\n",
        "# ================================================\n",
        "def chunk_text(text, chunk_size=500, overlap=50):\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "# Apply chunking to all extracted documents\n",
        "all_chunks = []\n",
        "for doc in documents:\n",
        "    all_chunks.extend(chunk_text(doc))\n",
        "\n",
        "# ================================================\n",
        "# üß† Step 6: Embed the Chunks Using Sentence Transformers\n",
        "# - Converts each chunk into a vector for semantic search\n",
        "# - 'all-MiniLM-L6-v2' is a fast, general-purpose embedding model\n",
        "# ================================================\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = embed_model.encode(all_chunks)\n",
        "\n",
        "# ================================================\n",
        "# üß≠ Step 7: Build a FAISS Index for Fast Similarity Search\n",
        "# - FAISS lets us efficiently retrieve similar text chunks\n",
        "# ================================================\n",
        "import faiss\n",
        "\n",
        "dimension = embeddings.shape[1]  # Dimensionality of embedding vectors\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(embeddings))  # Add all embeddings to the index\n",
        "\n",
        "# ================================================\n",
        "# üßë‚Äçüíª Step 8: Set Up the OpenAI Client (v1.x SDK)\n",
        "# - Uses the API key stored in the environment\n",
        "# ================================================\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# ================================================\n",
        "# ü§ñ Step 9: Define the RAG Query Function\n",
        "# - Accepts a question and retrieves the top-k most relevant chunks\n",
        "# - Builds a prompt with retrieved context and sends to GPT-4\n",
        "# ================================================\n",
        "def rag_query(question, k=3):\n",
        "    # Embed the user question\n",
        "    query_embedding = embed_model.encode([question])\n",
        "\n",
        "    # Search the FAISS index for top-k relevant chunks\n",
        "    distances, indices = index.search(np.array(query_embedding), k)\n",
        "\n",
        "    # Retrieve top-k chunks and format them as context\n",
        "    context = \"\\n---\\n\".join([all_chunks[i] for i in indices[0]])\n",
        "\n",
        "    # Construct a simple prompt using the retrieved context\n",
        "    prompt = f\"\"\"Use the context below to answer the question. Be concise and accurate.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Call the OpenAI GPT model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",  # Change as needed (e.g., \"gpt-4\" or \"gpt-3.5-turbo\")\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2,  # Low temperature = more deterministic responses\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# ================================================\n",
        "# ‚ùì Step 10: Ask a Question to Your RAG System!\n",
        "# - You can change the question below to test your documents\n",
        "# ================================================\n",
        "print(rag_query(\"What are the key topics discussed in the uploaded document(s)?\"))"
      ]
    }
  ]
}