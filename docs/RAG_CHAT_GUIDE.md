# ğŸ¤– RAG Chat with Estel â€” How It Works

A deep dive into the architecture and components behind the RAG Chat demo.

---

## What is RAG?

**Retrieval-Augmented Generation (RAG)** is a technique that gives a language model access to your own documents when answering questions. Instead of relying only on what the model learned during training, RAG retrieves relevant passages from your data and includes them in the prompt â€” grounding the model's response in your content.

This solves two fundamental problems with vanilla LLMs:

1. **Knowledge cutoff** â€” LLMs don't know about your private documents
2. **Hallucination** â€” Without context, models may generate plausible but incorrect answers

---

## The RAG Pipeline â€” Step by Step

Here's what happens when you upload a document and ask a question in Estel:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        RAG PIPELINE                              â”‚
â”‚                                                                  â”‚
â”‚  ğŸ“„ Upload        âœ‚ï¸ Chunk         ğŸ”¢ Embed        ğŸ’¾ Store       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚  PDF/TXT         Split into      Convert to       Save in        â”‚
â”‚  files           1000-char       numerical         FAISS          â”‚
â”‚                  pieces          vectors           index          â”‚
â”‚                                                                  â”‚
â”‚  â“ Query         ğŸ” Retrieve     ğŸ“ Augment       ğŸ’¬ Generate    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â†’ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚
â”‚  User's          Find top        Add chunks       LLM answers    â”‚
â”‚  question        matching        to the           using the      â”‚
â”‚                  chunks          prompt           context         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Deep Dive

### 1. Document Loading (`rag_utils.py`)

When you upload files, Estel uses LangChain's document loaders:

- **PDFs** â†’ `PyPDFLoader` â€” extracts text page by page
- **TXT files** â†’ `TextLoader` â€” reads the raw text content

Each file becomes a list of `Document` objects containing the text and metadata (page number, source file name).

```python
# Under the hood
loader = PyPDFLoader("uploaded_file.pdf")
docs = loader.load()
# docs = [Document(page_content="...", metadata={page: 0, source: "..."}), ...]
```

### 2. Chunking â€” `RecursiveCharacterTextSplitter`

Entire documents are too large to send to an LLM. Chunking splits them into smaller, overlapping pieces:

| Parameter | Value | Why |
|-----------|-------|-----|
| `chunk_size` | 1000 characters | Large enough to contain a complete thought |
| `chunk_overlap` | 200 characters | Prevents cutting a sentence in the middle |

**How `RecursiveCharacterTextSplitter` works:**

It tries to split on natural boundaries in this order:
1. Double newlines (`\n\n`) â€” paragraph breaks
2. Single newlines (`\n`) â€” line breaks
3. Spaces â€” word boundaries
4. Characters â€” last resort

This preserves meaning better than splitting at arbitrary character counts.

```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(docs)
```

**Example:** A 5-page PDF might produce 15-25 chunks, each ~1000 characters with 200 characters overlapping between consecutive chunks.

### 3. Embeddings â€” `text-embedding-ada-002`

Each chunk is converted into a **numerical vector** (a list of 1536 numbers) that captures its semantic meaning. This is done by OpenAI's embedding model.

**Key insight:** Text with similar meaning produces vectors that are close together in vector space, regardless of the exact words used.

| Text | Similar? |
|------|----------|
| "The cat sat on the mat" â†” "A feline rested on the rug" | âœ… Close vectors |
| "The cat sat on the mat" â†” "Stock market crashed today" | âŒ Far apart |

```python
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")
# Each chunk â†’ [0.0023, -0.0142, 0.0341, ...] (1536 dimensions)
```

### 4. Vector Store â€” FAISS

**FAISS** (Facebook AI Similarity Search) is an open-source library for fast similarity search over vectors. It stores all chunk embeddings in an optimized index.

**Why FAISS?**
- Extremely fast â€” searches millions of vectors in milliseconds
- Runs locally â€” no external database needed
- Simple API â€” `from_documents()` to create, `as_retriever()` to search
- Persistent â€” saves to disk, reloads instantly

```python
# Create the index
vectordb = FAISS.from_documents(chunks, embeddings)

# Save to disk
vectordb.save_local("vectorstore_index/")

# Reload later
vectordb = FAISS.load_local("vectorstore_index/", embeddings)
```

**What's on disk:**

```
vectorstore_index/
â”œâ”€â”€ index.faiss    â† The vector index (binary, optimized for search)
â””â”€â”€ index.pkl      â† Mapping from vector IDs back to document text
```

### 5. Retrieval â€” Finding Relevant Chunks

When you ask a question, Estel:
1. Converts your question into a vector (same embedding model)
2. Searches the FAISS index for the most similar chunk vectors
3. Returns the top-k matching chunks (default: 4)

This is **semantic search** â€” it finds chunks by meaning, not keywords. So asking "What were the company's profits?" would match a chunk containing "Revenue exceeded $10M with net earnings of $2M" even though no words overlap.

```python
retriever = vectordb.as_retriever()
# Internally: embed question â†’ find nearest vectors â†’ return chunks
```

### 6. Augmented Generation â€” `RetrievalQA` Chain

LangChain's `RetrievalQA` chain ties everything together:

1. **Retrieve** â€” Get relevant chunks from FAISS
2. **Augment** â€” Insert chunks into a prompt template as context
3. **Generate** â€” Send the augmented prompt to `gpt-4o-mini`

```python
prompt_template = """You are a helpful assistant. Use the context below
to answer the user's question.

Context:
{context}

Question: {question}

Answer:"""

chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4o-mini", temperature=0),
    retriever=vectorstore.as_retriever(),
    chain_type="stuff",        # "stuff" = put all chunks in one prompt
    chain_type_kwargs={"prompt": prompt}
)
```

**Chain type "stuff"** means all retrieved chunks are concatenated ("stuffed") into a single prompt. This is simple and works well when retrieved content fits within the context window. Other strategies like "map_reduce" or "refine" handle larger contexts but add complexity.

---

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMLIT UI                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Upload &    â”‚   Ask        â”‚   Chat                â”‚  â”‚
â”‚  â”‚  Index Tab   â”‚   Estel Tab  â”‚   History Tab         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚              â”‚                                  â”‚
â”‚         â–¼              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚ rag_utils   â”‚ â”‚chat_utilsâ”‚                            â”‚
â”‚  â”‚ .py         â”‚ â”‚ .py      â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚         â”‚              â”‚                                  â”‚
â”‚         â–¼              â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚         LangChain                â”‚                     â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                     â”‚
â”‚  â”‚  â”‚TextSplit  â”‚  â”‚RetrievalQA   â”‚ â”‚                     â”‚
â”‚  â”‚  â”‚Embeddings â”‚  â”‚PromptTemplateâ”‚ â”‚                     â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚           â”‚                â”‚                              â”‚
â”‚           â–¼                â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚   FAISS      â”‚  â”‚  OpenAI API  â”‚                      â”‚
â”‚  â”‚  (local)     â”‚  â”‚  gpt-4o-mini â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
estel/
â”œâ”€â”€ __init__.py       â† Package exports
â”œâ”€â”€ constants.py      â† Configuration (model names, paths)
â”œâ”€â”€ rag_utils.py      â† Document loading, chunking, FAISS create/load
â””â”€â”€ chat_utils.py     â† RetrievalQA chain creation

pages/
â””â”€â”€ 3_RAG_Chat.py     â† Streamlit UI with 3 tabs
```

| File | Responsibility |
|------|---------------|
| `constants.py` | Model names (`gpt-4o-mini`, `text-embedding-ada-002`), vector store path |
| `rag_utils.py` | `load_documents()` â€” parse PDFs/TXT; `create_vectorstore()` â€” chunk, embed, index; `load_vectorstore()` â€” reload from disk |
| `chat_utils.py` | `get_chain()` â€” builds the LangChain RetrievalQA chain with prompt template |
| `3_RAG_Chat.py` | Three-tab Streamlit interface: Upload & Index, Ask Estel, Chat History |

---

## Key Concepts for Students

### Why chunk size matters
- **Too small** (100 chars) â€” Chunks lack context, retrieval returns fragments
- **Too large** (5000 chars) â€” Less precise retrieval, wastes context window
- **Sweet spot** (~1000 chars) â€” Enough context per chunk while maintaining precision

### Why overlap matters
- Without overlap, a key sentence split across two chunks would be lost
- 200-character overlap ensures boundary sentences appear in both chunks

### Temperature = 0
- Estel uses `temperature=0` for deterministic, factual responses
- Higher temperatures would introduce randomness â€” not ideal for document Q&A

### "Stuff" chain type
- Simplest approach: concatenate all retrieved chunks into one prompt
- Works when total retrieved text fits in the model's context window
- For very large documents, alternatives like `map_reduce` process chunks in batches

### FAISS vs. cloud vector databases
| Feature | FAISS (used here) | Pinecone / Weaviate |
|---------|-------------------|---------------------|
| Cost | Free | Paid (managed service) |
| Setup | `pip install faiss-cpu` | Account + API keys |
| Persistence | Local files | Cloud-hosted |
| Scale | Millions of vectors | Billions |
| Best for | Prototyping, education | Production, multi-user |

---

## Try These Experiments

1. **Upload a PDF and ask questions about it** â€” Start simple, then ask follow-up questions
2. **Upload two contrasting documents** â€” Ask about differences between them
3. **Ask a question NOT covered in the document** â€” See how the model handles it (it should say it doesn't know)
4. **Compare RAG vs. no-RAG** â€” Ask the same question to ChatGPT without the document context and compare answers

---

## Cost Notes

Each question involves two API calls:
1. **Embedding the question** â€” ~$0.0001 per query (negligible)
2. **LLM generation** â€” `gpt-4o-mini` at ~$0.15/1M input tokens + $0.60/1M output tokens

A typical question with 4 retrieved chunks costs roughly **$0.001â€“0.003** (a fraction of a cent).

Document indexing is a one-time cost per document upload:
- Embedding 50 chunks â‰ˆ $0.001

---

*Part of GenAI Foundry â€” MIT Professional Education: Applied Generative AI for Digital Transformation*
